NB.prob <- predict(NB.fit, as.matrix(X.test),type="raw")
head(NB.prob)
hist(NB.prob)
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
X.train.bin = apply(X.train, MARGIN = 2, convert_counts)
X.test.bin = apply(X.test, MARGIN = 2, convert_counts)
NB.fit <- naiveBayes(X.train.bin, y.train, laplace = 1) # laplace controls smoothing of probabilities
NB.pred <- predict(NB.fit, X.test.bin)
# compute the confusion matrix
#Kappa is Accuracy - Naive Prediction
confusionMatrix(NB.pred,y.test)
#Uses a multinomila dist for each column instead of a normal distribution
NB.fit <- multinomial_naive_bayes(as.matrix(X.train), y.train, laplace=.6)
NB.pred <- predict(NB.fit, as.matrix(X.test))
NB.prob <- predict(NB.fit, as.matrix(X.test),type="prob")
hist(NB.prob)
#confusion matrix
confusionMatrix(NB.pred,y.test)
NB.fit <- bernoulli_naive_bayes(as.matrix(X.train), y.train, laplace=0.2)
NB.pred <- predict(NB.fit, as.matrix(X.test))
NB.prob <- predict(NB.fit, as.matrix(X.test),type="prob")
hist(NB.prob)
#confusion matrix
confusionMatrix(NB.pred,y.test)
names(getModelInfo())
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
number = 10)
trctrl <- trainControl(method = "none")
X.train = X.train %>% as.matrix() %>% as.data.frame()
X.test = X.test %>% as.matrix() %>% as.data.frame()
X.train.bin = X.train.bin %>% as.matrix() %>% as.data.frame()
X.test.bin = X.test.bin %>% as.matrix() %>% as.data.frame()
nb_mod <- train(
x = X.train.bin
, y = y.train
, method = "naive_bayes"
, trControl = trctrl
, tuneGrid = data.frame(
laplace = 0.5 #number to smooth the probabilities
, usekernel = FALSE #non linear distance
, adjust = FALSE
)
)
nb_pred <- predict(nb_mod,
newdata = X.test.bin)
confusionMatrix(nb_pred,y.test)
#Optimize de Hyper Parameters
ctrl = trainControl(
method = "repeatedcv"
, repeats = 1
, number = 10
, verboseIter = T
)
#Caret will use all possible combination of these grid  to select the best one
nb_grid = expand.grid(
usekernel = c(TRUE, FALSE)
, laplace = c(0, 0.5, 1)
, adjust = c(0.5, 1, 1.5)
)
nb_mod <- train(x = X.train.bin,
y = y.train,
method = "naive_bayes",
trControl = ctrl,
tuneGrid = nb_grid)
nb_pred <- predict(nb_mod,
newdata = X.test.bin)
confusionMatrix(nb_pred,y.test)
plot(nb_mod)
plot(confusionMatrix(nb_pred,y.test)[["table"]])
wine.data$type = as.factor(wine.data$type)
wine.data$type
as.numeric(wine.data$type)
wine.data$type %>% as.factor()
wine.data$type %>% as.factor() %>% as.numeric()
#Graph type count
ggplot(
training.set
, aes(x = type, fill = type)
) +
geom_bar(stat = "count") +
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph type count
ggplot(
training.set
, aes(x = type, fill = type)
) +
geom_bar(stat = "count") +
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph Quality Dist
ggplot(
training.set
, aes(x = quality, fill = type)
) +
geom_bar(stat = "count", position = "stack")+
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph
ggplot(
melt(training.set %>% mutate(quality = as.factor(quality)))
, aes(
y = value
, x = quality
, col = quality
)
) +
geom_boxplot() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(x = value, col = type)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set %>% mutate(quality = as.factor(quality)))
, aes(x = value, col = quality)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
table(training.set$quality)
round(prop.table(table(training.set$quality)),4)
library(tidyverse)
library(MASS)
library(caret)
library(e1071)
library(tm)
library(wordcloud)
library(SnowballC)
library(naivebayes)
library(ggplot2)
library(GGally)
library(reshape2)
library(klaR)
#Data Upload
wine.data = read.csv("wine-quality-white-and-red.csv")
#Data Description
dim(wine.data)
str(wine.data)
wine.data$type = wine.data$type %>% as.factor() %>% as.numeric()
wine.data$quality = as.factor(wine.data$quality)
#Partition between Training and Test set (Training 80% - Test 20%)
part.ind = createDataPartition(wine.data$type, p = 0.8, list = F)
training.set = wine.data[part.ind,]
test.set = wine.data[-part.ind,]
summary(training.set)
devtools::install_github("LudvigOlsen/splitChunk")
#Graph type count
ggplot(
training.set
, aes(x = type, fill = type)
) +
geom_bar(stat = "count") +
geom_text(stat='count', aes(label=..count..), vjust=-1)
library(tidyverse)
library(MASS)
library(caret)
library(e1071)
library(tm)
library(wordcloud)
library(SnowballC)
library(naivebayes)
library(ggplot2)
library(GGally)
library(reshape2)
library(klaR)
#Data Upload
wine.data = read.csv("wine-quality-white-and-red.csv")
#Data Description
dim(wine.data)
str(wine.data)
wine.data$type = wine.data$type %>% as.factor() %>% as.numeric()
wine.data$quality = as.factor(wine.data$quality)
#Partition between Training and Test set (Training 80% - Test 20%)
part.ind = createDataPartition(wine.data$type, p = 0.8, list = F)
training.set = wine.data[part.ind,]
test.set = wine.data[-part.ind,]
summary(training.set)
#Graph type count
ggplot(
training.set
, aes(x = type, fill = type)
) +
geom_bar(stat = "count") +
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph Quality Dist
ggplot(
training.set
, aes(x = quality, fill = type)
) +
geom_bar(stat = "count", position = "stack")+
geom_text(stat='count', aes(label=..count..), vjust=-1)
training.set
#Graph Quality Dist
ggplot(
training.set
, aes(
x = quality
, col = type)
) +
geom_bar(stat = "count", position = "stack")+
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph Quality Dist
ggplot(
training.set
, aes(
x = quality
, col = as.factor(type))
) +
geom_bar(stat = "count", position = "stack")+
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph Quality Dist
ggplot(
training.set
, aes(
x = quality
)
) +
geom_bar(stat = "count", position = "stack")+
geom_text(stat='count', aes(label=..count..), vjust=-1)
table(training.set$quality)
round(prop.table(table(training.set$quality)),4)
#Graph Quality Dist
ggplot(
training.set
, aes(
x = quality
)
) +
geom_bar(stat = "count", position = "stack")+
geom_text(stat='count', aes(label=..count..), vjust=-1)
#Graph
ggplot(
melt(training.set)
, aes(
y = value
, x = quality
, col = quality
)
) +
geom_boxplot() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(
y = value
, x = quality
, col = quality
)
) +
geom_boxplot() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(x = value, col = type)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(x = value)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set %>% mutate(quality = as.factor(quality)))
, aes(x = value, col = quality)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(
y = value
, x = quality
, col = quality
)
) +
geom_boxplot() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(
y = value
, x = quality
, col = quality
)
) +
geom_boxplot() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set %>% mutate(quality = as.factor(quality)))
, aes(x = value, col = quality)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set)
, aes(
y = value
, x = quality
, col = quality
)
) +
geom_boxplot() +
facet_wrap(~variable, scales = "free")
#Graph
ggplot(
melt(training.set %>% mutate(quality = as.factor(quality)))
, aes(x = value, col = quality)
) +
geom_density() +
facet_wrap(~variable, scales = "free")
#Graph
pairs(iris[,1:4], pch = 19)
#Graph
pairs(training.set)
#Graph
pairs(training.set[,c(-1,-13)])
dim(training.set$quality)
dim(training.set)
library(VGAM)
log.fit = vglm(quality ~ ., family = multinomial(refLevel = 1), training.set)
library(VGAM)
log.fit = vglm(quality ~ ., family = multinomial(refLevel = 1), training.set)
summary(log.fit)
prob.test = predict(log.fit, newdata=test.set, type="response")
pred.test = as.factor(levels(wine.data$quality)[max.col(prob.test)])
head(pred.test)
levels(pred.test) = levels(wine.data$quality)
confusionMatrix(pred.test, test.set$quality)$table
confusionMatrix(pred.test, test.set$quality)$overall[1]
prob.test = predict(log.fit, newdata=test.set, type="response")
pred.test = as.factor(levels(wine.data$quality)[max.col(prob.test)])
head(pred.test)
levels(pred.test) = levels(wine.data$quality)
confusionMatrix(pred.test, test.set$quality)$table
confusionMatrix(pred.test, test.set$quality)$overall[1]
lda.class <- lda(quality ~ ., training.set)
post.prob.lda = predict(lda.class, test.set)$posterior
pred.lda = predict(lda.class, test.set)$class
n = dim(test.set)[1]
ConfMat.lda = table(pred.lda, test.set$quality)
ConfMat.lda
lda.class <- lda(quality ~ ., training.set)
post.prob.lda = predict(lda.class, test.set)$posterior
pred.lda = predict(lda.class, test.set)$class
n = dim(test.set)[1]
ConfMat.lda = table(pred.lda, test.set$quality)
ConfMat.lda
error.lda <- (n - sum(diag(ConfMat.lda))) / n
error.lda
nb.class <- naive_bayes(quality ~ ., training.set)
pred.nb = predict(nb.class, test.set)
colors.nb.iris.good.bad <- c("black","red")[1*(test.set[,13]==pred.nb)+1]
n = dim(test.set)[1]
ConfMat.nb = table(pred.nb, test.set$quality)
ConfMat.nb
nb.class <- naive_bayes(quality ~ ., training.set)
pred.nb = predict(nb.class, test.set)
n = dim(test.set)[1]
ConfMat.nb = table(pred.nb, test.set$quality)
ConfMat.nb
error.nb <- (n - sum(diag(ConfMat.nb))) / n
error.nb
nb.class <- naive_bayes(quality ~ ., training.set)
pred.nb = predict(nb.class, test.set)
n = dim(test.set)[1]
ConfMat.nb = table(pred.nb, test.set$quality)
ConfMat.nb
error.nb <- (n - sum(diag(ConfMat.nb))) / n
error.nb
training.set %>%
mutate(bin.quality = ifelse(as.numeric(quality)>6, "Good Wine", "Bad Wine")) %>%
select(-quality)
training.set %>%
dplyr::mutate(bin.quality = ifelse(as.numeric(quality)>6, "Good Wine", "Bad Wine")) %>%
dplyr::select(-quality)
ifelse(as.numeric(quality) > 6, "Good Wine", "Bad Wine")
as.numeric(quality) > 6
as.numeric(training.set$quality) > 6
table(as.numeric(training.set$quality) > 6)
as.numeric(training.set$quality)
table(training.set$quality)
table(as.numeric(training.set$quality))
table(training.set$quality)
training.set %>%
dplyr::mutate(bin.quality = ifelse(training.set$quality %in% c("7","8","9","10"), "Good Wine", "Bad Wine")) %>%
dplyr::select(-quality)
training.set$quality %in% c("7","8","9","10")
table(training.set$quality %in% c("7","8","9","10"))
table(training.set$quality %in% c("6","7","8","9","10"))
training.set.bin = training.set %>%
dplyr::mutate(bin.quality = ifelse(quality %in% c("6","7","8","9","10"), "Good Wine", "Bad Wine")) %>%
dplyr::select(-quality)
training.set.bin
table(training.set.bin$bin.quality)
#Logistic Regression
bin.log.fit = vglm(bin.quality ~ ., family = multinomial(refLevel = 1), training.set.bin)
#Logistic Regression
bin.log.fit = vglm(bin.quality ~ ., family = multinomial(refLevel = 1), training.set.bin)
#LDA
bin.lda.class <- lda(bin.quality ~ ., training.set.bin)
#QDA
bin.qda.class <- qda(bin.quality ~ ., training.set.bin)
#Naive Bayes
bin.nb.class <- naive_bayes(bin.quality ~ ., training.set.bin)
pred.test = as.factor(levels(training.set..bin$quality)[max.col(prob.test)])
pred.test = as.factor(levels(training.set.bin$quality)[max.col(prob.test)])
head(pred.test)
test.set.bin = test.set %>%
dplyr::mutate(bin.quality = ifelse(quality %in% c("6","7","8","9","10"), "Good Wine", "Bad Wine")) %>%
dplyr::mutate(bin.quality = as.factor(bin.quality)) %>%
dplyr::select(-quality)
prob.test = predict(log.fit, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$quality)[max.col(prob.test)])
head(pred.test)
prob.test = predict(log.fit, newdata=test.set.bin, type="response")
prob.test
levels(training.set.bin$quality)
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
pred.test
levels(training.set.bin$bin.quality)
training.set.bin$bin.quality
training.set.bin
training.set.bin$bin.quality = as.factor(training.set.bin$bin.quality)
test.set.bin$bin.quality = as.factor(test.set.bin$bin.quality)
prob.test = predict(log.fit, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
head(pred.test)
training.set.bin$bin.quality
levels(training.set.bin$bin.quality)
prob.test = predict(log.fit, newdata=test.set.bin, type="response")
prob.test
prob.test = predict(bin.log.fit, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
head(pred.test)
prob.test = predict(bin.log.fit, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
confusionMatrix(pred.test, test.set$bin.quality)$table
prob.test = predict(bin.log.fit, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
confusionMatrix(pred.test, test.set.bin$bin.quality)$table
confusionMatrix(pred.test, test.set.bin$bin.quality)$overall[1]
prob.test = predict(bin.log.fit, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
confusionMatrix(pred.test, test.set.bin$bin.quality)$table
confusionMatrix(pred.test, test.set.bin$bin.quality)$overall[1]
prob.test = predict(bin.lda.class, newdata=test.set.bin, type="response")
pred.test = as.factor(levels(training.set.bin$bin.quality)[max.col(prob.test)])
prob.test = predict(bin.lda.class, newdata=test.set.bin, type="response")
pred.test = prob.test$class
confusionMatrix(pred.test, test.set.bin$bin.quality)$table
confusionMatrix(pred.test, test.set.bin$bin.quality)$overall[1]
prob.test = predict(bin.qda.class, newdata=test.set.bin, type="response")
pred.test = prob.test$class
confusionMatrix(pred.test, test.set.bin$bin.quality)$table
confusionMatrix(pred.test, test.set.bin$bin.quality)$overall[1]
pred.nb = predict(bin.nb.class, test.set.bin)
n = dim(test.set.bin)[1]
pred.nb = predict(bin.nb.class, test.set.bin)
pred.nb = predict(bin.nb.class, test.set.bin)
n = dim(test.set.bin)[1]
ConfMat.nb = table(pred.nb, test.set.bin$bin.quality)
ConfMat.nb
error.nb <- (n - sum(diag(ConfMat.nb))) / n
error.nb
error.nb <- sum(diag(ConfMat.nb)) / n
error.nb
ConfMat.nb = table(pred.nb, test.set$quality)
ConfMat.nb
error.nb <- sum(diag(ConfMat.nb)) / n
error.nb <- sum(diag(ConfMat.nb)) / n
error.nb <- sum(diag(ConfMat.nb)) / n
error.nb
nb.class <- naive_bayes(quality ~ ., training.set)
pred.nb = predict(nb.class, test.set)
n = dim(test.set)[1]
ConfMat.nb = table(pred.nb, test.set$quality)
ConfMat.nb
error.nb <- sum(diag(ConfMat.nb)) / n
error.nb
pred.nb = predict(bin.nb.class, test.set.bin)
n = dim(test.set.bin)[1]
ConfMat.nb = table(pred.nb, test.set.bin$bin.quality)
ConfMat.nb
error.nb <- sum(diag(ConfMat.nb)) / n
error.nb
pred.nb = predict(bin.nb.class, test.set.bin)
n = dim(test.set.bin)[1]
ConfMat.nb = table(pred.nb, test.set.bin$bin.quality)
ConfMat.nb
accu.nb <- sum(diag(ConfMat.nb)) / n
accu.nb
table(training.set.bin$bin.quality)
props.table(table(training.set.bin$bin.quality))
prop.table(table(training.set.bin$bin.quality))
```{r, echo = F}
library(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
data = read.csv("wine-quality-white-and-red.csv")
